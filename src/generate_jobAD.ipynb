{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shilpi/Documents/P2_BiasMitigation_DataAugmentation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shilpi/Documents/P2_BiasMitigation_DataAugmentation/.venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.29it/s]\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated responses saved to generated_responses.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the CSV file containing sample bios\n",
    "bio_df = pd.read_csv(\"accountant_samples.csv\") #path to Bios sample \n",
    "\n",
    "# Read the job ad prompt from a text file\n",
    "with open(\"Accountant.txt\", \"r\") as f:  #path to job ad prompt text file \n",
    "    job_ad_prompt = f.read().strip()\n",
    "\n",
    "# Load the LLaMA model with memory optimization\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",  \n",
    "    torch_dtype=torch.float16, # Use FP16 to reduce memory\n",
    "     device_map=\"auto\"  # Automatically allocate to available GPU/CPU\n",
    ")\n",
    "\n",
    "# Generate text for each bio\n",
    "generated_texts = []\n",
    "for bio in bio_df[\"hard_text\"]:  # bio text column\n",
    "    input_text = f\"{job_ad_prompt}\\n\\n{bio}\"  # Combine job ad and bio\n",
    "    output = pipe(input_text, max_length=512, num_return_sequences=1)\n",
    "    generated_texts.append(output[0][\"generated_text\"])\n",
    "\n",
    "# Save results to a new CSV file\n",
    "bio_df[\"generated_text\"] = generated_texts\n",
    "bio_df.to_csv(\"generated_job_AD_responses.csv\", index=False)\n",
    "\n",
    "print(\"Generated responses saved to generated_responses.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
