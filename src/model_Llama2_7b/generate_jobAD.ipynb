{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model meta-llama\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "\n",
    "# Load the CSV file containing sample bios\n",
    "bio_df = pd.read_csv(\"accountant_samples.csv\") #path to Bios sample \n",
    "\n",
    "# Read the job ad prompt from a text file\n",
    "with open(\"prompt_template_jobAD.txt\", \"r\") as f:  #path to job ad prompt text file \n",
    "    job_ad_prompt = f.read()\n",
    "\n",
    "# Load the LLaMA model with text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    "   # device=0 if torch.cuda.is_available() else -1  \n",
    ")\n",
    "\n",
    "# --- Generate Outputs ---\n",
    "def format_prompt(template, bio, profession, gender):\n",
    "    return f\"<s>[INST] {template.format(bio=bio, gender=gender, profession=profession)} [/INST]\"      #The Llama-2-7b-chat-hf model uses a chat template, so wrap your prompts in [INST] ... [/INST]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text for each bio (max_new_tokens=512))\n",
    "generated_texts = []\n",
    "for i, row in bio_df.iterrows():  \n",
    "    bio = row[\"hard_text\"]      # bio text column\n",
    "    profession = row[\"profession\"]\n",
    "    gender = row[\"gender\"] \n",
    "    prompt = format_prompt(job_ad_prompt, bio, profession, gender)\n",
    "    \n",
    "    response = pipe(prompt, max_new_tokens=512, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "    generated_texts.append({\n",
    "        \"profession\": profession,\n",
    "        \"bio\": bio,\n",
    "        \"generated_text\": response\n",
    "    })\n",
    "    print(f\"\\n--- Result for Bio #{i+1} ---\\n{response}\\n\")\n",
    "\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "os.makedirs(\"job_ads/jobADs_accountant\", exist_ok=True)\n",
    "\n",
    "# Save each job ad as a separate file inside the folder\n",
    "for i, ad in enumerate(generated_texts, start=1):\n",
    "    filename = f\"job_ads/jobADs_accountant/job_ad_{i}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "            f.write(f\"\\n{ad['generated_text']}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text for each bio (max_new_tokens=2048)\n",
    "generated_texts = []\n",
    "for i, row in bio_df.iterrows():  \n",
    "    bio = row[\"hard_text\"]      # bio text column\n",
    "    profession = row[\"profession\"]\n",
    "    gender = row[\"gender\"] \n",
    "    prompt = format_prompt(job_ad_prompt, bio, profession, gender)\n",
    "    \n",
    "    response = pipe(prompt, max_new_tokens=2048, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "    generated_texts.append({\n",
    "        \"profession\": profession,\n",
    "        \"bio\": bio,\n",
    "        \"generated_text\": response\n",
    "    })\n",
    "    print(f\"\\n--- Result for Bio #{i+1} ---\\n{response}\\n\")\n",
    "\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "os.makedirs(\"job_ads/jobADs_len2048\", exist_ok=True)\n",
    "\n",
    "# Save each job ad as a separate file inside the folder\n",
    "for i, ad in enumerate(generated_texts, start=1):\n",
    "    filename = f\"job_ads/jobADs_len2048/job_ad_{i}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "            f.write(f\"\\n{ad['generated_text']}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
